<div align="center">

  <img src=".github/assets/logo.png" alt="BrazucaTalks Logo" width="250" height="auto" />

  # BRAZUCATALKS
  
  ### Distributed Frugal AI Ecosystem for Multimodal Tutoring
  
  <!-- ANIMATED TYPING EFFECT -->
  <a href="https://git.io/typing-svg">
    <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&weight=600&size=22&pause=1000&color=2E7D32&center=true&vCenter=true&width=600&lines=Cambridge+Frugal+AI+Framework+Implementation;0.08s+Latency+via+Semantic+Caching;Multimodal+GenAI+on+Core+i3+Hardware;Democratic+and+Sovereign+Education" alt="Typing SVG" />
  </a>

  <!-- PROFESSIONAL BADGES -->
  <p>
    <img src="https://img.shields.io/badge/Architecture-Distributed_Edge_AI-blue?style=for-the-badge&logo=google-cloud" />
    <img src="https://img.shields.io/badge/Status-Elite_MVP-success?style=for-the-badge" />
    <img src="https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge" />
  </p>

  <p>
    <a href="#-about-the-project">About</a> ‚Ä¢
    <a href="#-sota-architecture">Architecture</a> ‚Ä¢
    <a href="#-tech-stack">Tech Stack</a> ‚Ä¢
    <a href="#-performance-benchmarks">Performance</a> ‚Ä¢
    <a href="#-getting-started">Getting Started</a>
  </p>
</div>



## üí° About the Project

BrazucaTalks is a high-performance reference implementation of the Frugal AI Ecosystem theoretical framework, recently proposed by the Cambridge Judge Business School (Nov 2025).
While the current AI paradigm remains tethered to multi-billion dollar data centers and unsustainable energy footprints, BrazucaTalks proves that a sophisticated, multimodal, and context-aware AI can thrive on commodity hardware. Developed on a standard Intel Core i3 with 8GB of RAM, this project serves as a bridge between high-level AI research and practical, democratic accessibility in the Global South.
üéØ The Core Challenge
State-of-the-Art (SOTA) Large Language Models (LLMs) often suffer from high latency and prohibitive operational costs. In a distributed systems context, these bottlenecks prevent the scaling of personalized education. BrazucaTalks solves the "AI Trilemma" (Cost, Latency, and Context) through a distributed modular architecture.
üöÄ Key Innovations
Semantic Short-Circuiting: By implementing a Semantic Cache Layer using vector similarity search in Redis Stack, the system bypasses heavy neural inference for recurring queries, reducing response latency from ~100s to 0.08s (a 1250x performance boost).
Hybrid Intent Routing: A custom Semantic Router employs a heuristic-neural hybrid approach (Cosine Similarity + Weighted Keyword Boosting) to classify user intents in milliseconds, ensuring that expensive RAG pipelines are only activated when technically necessary.
Distributed Statelessness: The backend is strictly stateless, delegating session management and conversation history to an external Redis instance. This architectural choice enables seamless horizontal scaling and high availability.
Multimodal Edge Intelligence: Integration of quantized SLMs (Small Language Models), int8-quantized STT (Faster-Whisper), and Real-time Lip-Sync via Web Audio API, providing a human-like tutoring experience without any reliance on paid cloud APIs.
üåç Impact & Relevance
This project demonstrates that Data Sovereignty and Privacy-First AI are achievable for public institutions and schools with limited resources. It stands as a testament to Frugal Engineering: the art of delivering "State-of-the-Art" results through architectural precision rather than brute-force hardware.


## üèõÔ∏è SOTA Architecture

The system employs a **Stateless Distributed Architecture**, orchestrated for maximum resource efficiency.

```mermaid
graph TD
    User((ü¶Å Student)) -->|Voice/Text| Frontend[‚öõÔ∏è React + Three.js]
    Frontend -->|REST/Stream| API[üêç FastAPI Gateway]
    
    subgraph "Edge Brain (Core i3)"
        API --> Orchestrator{ChatService Maestro}
        
        Orchestrator -->|1. Check| Cache[‚ö° Semantic Cache]
        Cache -.->|Hit 0.08s| API
        
        Orchestrator -->|2. Miss| Router[üß† Semantic Router]
        
        Router -->|Technical| RAG[üìö Hybrid RAG]
        Router -->|Social| LLM[ü§ñ Quantized LLM]
        
        RAG <--> VectorDB[(Redis Stack)]
        Memory[(Session Memory)] <--> VectorDB
        
        Orchestrator --> Memory
    end
```

## Engineering Highlights:
- **Semantic Caching:** Utilizes Vector Search (Cosine Similarity) to identify recurring intents and provide instant  responses, bypassing heavy LLM inference.
- **Hybrid RAG:** Integrated pedagogical knowledge retrieval via PDF/JSON using HNSW indexing in Redis Stack.
- **Linguistic Sovereignty:** Advanced prompt engineering that enforces language policy and prevents persona leaking.

---

## üõ†Ô∏è Tech Stack

<div align="center">

| Category | Technologies |
| :--- | :--- |
| **Backend** | ![Python](https://img.shields.io/badge/Python_3.11-3776AB?style=flat&logo=python&logoColor=white) ![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=flat&logo=fastapi&logoColor=white) ![uv](https://img.shields.io/badge/uv-Manager-purple?style=flat) |
| **Frontend** | ![React](https://img.shields.io/badge/React_19-20232A?style=flat&logo=react&logoColor=61DAFB) ![Vite](https://img.shields.io/badge/Vite-646CFF?style=flat&logo=vite&logoColor=white) ![Tailwind](https://img.shields.io/badge/Tailwind_v4-38B2AC?style=flat&logo=tailwind-css&logoColor=white) |
| **AI & Data** | ![Ollama](https://img.shields.io/badge/Ollama-Local_AI-black?style=flat) ![Redis](https://img.shields.io/badge/Redis_Stack-DC382D?style=flat&logo=redis&logoColor=white) ![Three.js](https://img.shields.io/badge/Three.js-Avatar-black?style=flat&logo=three.js&logoColor=white) |

</div>

---

## üìä Performance Benchmarks

Real-world metrics captured on a consumer laptop (**Dell Inspiron, i3-1215U, 8GB RAM**):

| Metric | Result | Impact |
| :--- | :--- | :--- |
| **Cache Miss (Generation)** | ~60.0s | Heavy Neural Processing (LLM). |
| **Cache Hit (Semantic)** | **0.08s** | **850x faster.** Zero CPU cost. |
| **Intent Detection** | 0.01s | Mathematical Router (Linear Algebra). |
| **Memory Footprint** | Stable | No OOM Killer (Quantization & ZRAM). |

---

## üöÄ Getting Started

### Prerequisites
*   **Docker & Docker Compose**
*   **Ollama** (with `qwen2.5:1.5b` and `nomic-embed-text` models)
*   **Python 3.11+** (Recommended: [uv](https://github.com/astral-sh/uv))
*   **Node.js 20+**


## Setup Instructions

<details>
<summary><b>1. Infrastructure Setup</b> (Click to expand)</summary>


```Bash
# Start the Vector Database
docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest

# Pull AI Models
ollama pull qwen2.5:1.5b
ollama pull nomic-embed-text
```

</details>

<details>
<summary><b>2. Backend Initialization</b> (Click to expand)</summary>

```Bash
# In the project root
uv sync
uv run python -m src.app.rag.ingest_data  # Load knowledge base
PYTHONPATH=src uv run uvicorn app.main:app --reload
```

</details>

<details>
<summary><b>3. Frontend Initialization</b> (Click to expand)</summary>

```Bash
cd frontend
npm install
npm run dev
```
</details>

üë®‚Äçüíª Author
<div align="center">
Yuri Matheus
Undergraduate Researcher & Software Architect
IFNMG - Federal Institute of Northern Minas Gerais
![alt text](https://img.shields.io/badge/LinkedIn-Connect-blue?style=flat&logo=linkedin)

![alt text](https://img.shields.io/badge/Email-Contact-red?style=flat&logo=gmail)
</div>
This project was developed as a reference implementation for the Cambridge Frugal AI white paper.





