import logging
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware

import httpx

from src.app.core.logging import setup_logging
from src.app.api.routes import chat, audio
from src.app.core.config import settings
from src.app.api.routes import chat

# Configura√ß√£o de Logs
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("brazuka_core")

# Ciclo de vida (Lifespan)
@asynccontextmanager
async def lifespan(app: FastAPI):
  """
  Ciclo de vida da aplica√ß√£o.
  Executa antes de come√ßar a aceitar requisi√ß√µes.
  """
  logger.info(f"üöÄ Iniciando {settings.PROJECT_NAME} v{settings.VERSION} no ambiente {settings.ENV_MODE}")

  # 1. Smoke Test do Ollama (Verifica se a IA est√° rodando)
  try:
    async with httpx.AsyncClient() as client:
      resp = await client.get(f"{settings.OLLAMA_BASE_URL}/api/tags")
      if resp.status_code == 200:
        models = [m['name'] for m in resp.json()['models']]
        logger.info(f"‚úÖ Ollama Conectado! Modelos dispon√≠veis: {models}")

        # Verifica se o modelo escolhido est√° l√°
        if settings.MODEL_NAME not in str(models):
          logger.warning(f"‚ö†Ô∏è Modelo '{settings.MODEL_NAME}' n√£o encontrado no Ollama! Execute 'ollama pull {settings.MODEL_NAME}'")
      else:
        logger.error(f"‚ùå Ollama respondeu com erro: {resp.status_code}")
  except Exception as e:
    logger.critical(f"‚ùå FALHA CR√çTICA: N√£o foi poss√≠vel conectar ao Ollama em {settings.OLLAMA_BASE_URL}. Verifique se ele est√° rodando. Erro: {e}")

  yield

  logger.info("üõë Desligando aplica√ß√£o...")

setup_logging()

# Inicializa√ß√£o do App
app = FastAPI(
    title=settings.PROJECT_NAME,
    version=settings.VERSION,
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/static", StaticFiles(directory="static"), name="static")

# Registro de Rotas
app.include_router(chat.router, prefix=settings.API_V1_STR, tags=["Chat"])
app.include_router(audio.router, prefix=f"{settings.API_V1_STR}/audio", tags=["Audio"])

# Rota de Sa√∫de (Health Check)
@app.get("/health")
async def health_check():
  return {
      "status": "online",
      "app": settings.PROJECT_NAME,
      "model_target": settings.MODEL_NAME,
      "mode": "distributed_mvp"
  }

# Endpoint de teste r√°pido (s√≥ pra voc√™ ver a IA funcionando no navegador)
@app.get("/test-ai")
async def test_ai_connection():
    """Rota tempor√°ria para testar gera√ß√£o de texto"""
    import ollama
    try:
        # Nota: Em produ√ß√£o, isso ficar√° em app/services/llm.py
        response = ollama.chat(model=settings.MODEL_NAME, messages=[
            {'role': 'user', 'content': 'Diga "Ol√°, BrazucaTalks est√° online!" em ingl√™s.'},
        ])
        return {"response": response['message']['content']}
    except Exception as e:
        return {"error": str(e)}
